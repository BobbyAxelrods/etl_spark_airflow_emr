{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project: Find datasets\n",
    "\n",
    "## Purpose of this notebook\n",
    "\n",
    "I use this notebook to sketch ideas, test functions and build prototypes. The aim is to quickly implement ideas and use parts to build the actually used python scripts. Therefore, the notebook prioritizes quick implementation over standards or best practices. \n",
    "\n",
    "\n",
    "\n",
    "## Problem setting\n",
    "\n",
    "> A SME wants to access their sales data on a cloud computing platform to make further analyses and build a dashboard to inform managers.\n",
    "\n",
    "\n",
    "\n",
    "## Data \n",
    "\n",
    "1. Yahoo! Finance\n",
    "    1. get historical and daily data on certain industries\n",
    "2. Google trends\n",
    "    1. get weekly trends of the last 5 years\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage API Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../src/data')\n",
    "\n",
    "import helper_functions as h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### process_query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from datetime import datetime\n",
    "\n",
    "def handle_query_results(df_query_result, keywords, query_return_length=261):\n",
    "    \"\"\"Process query results: \n",
    "            (i) check for empty response --> create df with 0s if empty\n",
    "            (ii) drop isPartial rows and column\n",
    "            (iii) transpose dataframe to wide format (keywords//search interest)\n",
    "    \n",
    "    Input\n",
    "        df: dataframe containing query result (could be empty)\n",
    "        filename: name of temporary file\n",
    "        query_return_length: 261 is normal return length of query result \n",
    "        \n",
    "    Return\n",
    "        Dataframe: contains query results in long format \n",
    "        (rows: keywords, columns: search interest over time)\n",
    "    \"\"\"\n",
    "    # non-empty df\n",
    "    if df_query_result.shape[0] != 0:\n",
    "        # reset_index to preserve date information, drop isPartial column\n",
    "        df_query_result_processed = df_query_result.reset_index()\\\n",
    "            .drop(['isPartial'], axis=1)\n",
    "\n",
    "        df_query_result_long = pd.melt(df_query_result_processed, id_vars=['date'], var_name='keyword', value_name='search_interest')\n",
    "        \n",
    "        # long format (date, keyword, search interest)\n",
    "        return df_query_result_long\n",
    "\n",
    "    # no search result for any keyword: empty df\n",
    "    else:        \n",
    "        # create empty df with 0s\n",
    "        df_zeros = pd.DataFrame(np.zeros((query_return_length*len(keywords), 3)), columns=['date','keyword', 'search_interest'])\n",
    "        # replace 0s with keywords\n",
    "        df_zeros['keyword'] = np.repeat(keywords, query_return_length)\n",
    "\n",
    "        return df_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>keyword</th>\n",
       "      <th>search_interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>desirable Zoetis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>desirable Zoetis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>desirable Zoetis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>desirable Zoetis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>desirable Zoetis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>strong Zoetis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>0.0</td>\n",
       "      <td>strong Zoetis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>0.0</td>\n",
       "      <td>strong Zoetis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>0.0</td>\n",
       "      <td>strong Zoetis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>0.0</td>\n",
       "      <td>strong Zoetis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1305 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date           keyword  search_interest\n",
       "0      0.0  desirable Zoetis              0.0\n",
       "1      0.0  desirable Zoetis              0.0\n",
       "2      0.0  desirable Zoetis              0.0\n",
       "3      0.0  desirable Zoetis              0.0\n",
       "4      0.0  desirable Zoetis              0.0\n",
       "...    ...               ...              ...\n",
       "1300   0.0     strong Zoetis              0.0\n",
       "1301   0.0     strong Zoetis              0.0\n",
       "1302   0.0     strong Zoetis              0.0\n",
       "1303   0.0     strong Zoetis              0.0\n",
       "1304   0.0     strong Zoetis              0.0\n",
       "\n",
       "[1305 rows x 3 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zeros = pd.DataFrame() \n",
    "handle_query_results(df_zeros, keywords=['desirable Zoetis', 'resilient Zoetis', 'robust Zoetis', 'reasonable Zoetis', 'strong Zoetis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firm names\n",
    "### Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: ../data/processed/firm_namess.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_firms_sp500():\n",
    "    \"\"\"Obtain S&P 500 listings from Wikipedia\"\"\"\n",
    "    table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    df_sp500 = table[0]\n",
    "    \n",
    "    return df_sp500\n",
    "\n",
    "\n",
    "def regex_strip_legalname(raw_names):\n",
    "    \"\"\"Removes legal entity, technical description or firm type from firm name\n",
    "    \n",
    "    Input\n",
    "        raw_names: list of strings with firm names\n",
    "        \n",
    "    Return\n",
    "        list of strings: firm names without legal description \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pattern = r\"(\\s|\\.|\\,|\\&)*(\\.com|Enterprise|Worldwide|Int\\'l|N\\.V\\.|LLC|Co\\b|Inc\\b|Corp\\w*|Group\\sInc|Group|Company|Holdings\\sInc|\\WCo(\\s|\\.)|plc|Ltd|Int'l\\.|Holdings|\\(?Class\\s\\w+\\)?)\\.?\\W?\"\n",
    "    stripped_names = [re.sub(pattern,'', n) for n in raw_names]\n",
    "    \n",
    "    return stripped_names\n",
    "\n",
    "# get firm S&P500 from Wikipedia\n",
    "keep_columns = ['Symbol','Security', 'GICS Sector']\n",
    "df_sp500_wiki = get_firms_sp500().loc[:,keep_columns]\n",
    "\n",
    "# rename column, set ticker as index\n",
    "df_sp500_wiki= df_sp500_wiki.rename(columns={'Symbol': 'ticker', 'Security': 'firm_name_raw', 'GICS Sector': 'sector'})\n",
    "\n",
    "# process firm names (remove legal entity, suffix)\n",
    "df_sp500_wiki['firm_name_processed'] = regex_strip_legalname(list(df_sp500_wiki.firm_name_raw))\n",
    "\n",
    "# add retrieval date \n",
    "df_sp500_wiki['date_get_firmname'] = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# drop duplicate firm names \n",
    "df_sp500_wiki.drop_duplicates(subset='firm_name_processed', inplace=True)\n",
    "\n",
    "h.make_csv(df_sp500_wiki, 'firm_namess.csv', '../data/processed',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESG topics\n",
    "### Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 37 negative and 28 positive topics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scandal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>greenwashing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>corruption</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fraud</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bribe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic  positive\n",
       "0       scandal         0\n",
       "1  greenwashing         0\n",
       "2    corruption         0\n",
       "3         fraud         0\n",
       "4         bribe         0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "topics_negative = ['scandal', 'greenwashing', 'corruption', 'fraud', \n",
    "                   'bribe', 'tax', 'forced', 'harassment', 'violation', \n",
    "                   'illegal', 'conflict', 'weapons', 'pollution',\n",
    "                   'inequality', 'discrimination', 'sexism', 'racist', \n",
    "                   'intransparent', 'nontransparent', 'breach', 'lawsuit', \n",
    "                   'unfair', 'bad', 'problem', 'hate', 'issue', 'controversial', \n",
    "                  'strike', 'scam', 'trouble', 'controversy', 'mismanagement', \n",
    "                  'crisis', 'turmoil', 'shock', 'whistleblow', 'dispute']\n",
    "\n",
    "topics_positive =  ['green', 'sustainable', 'positive', 'best', 'good', \n",
    "                    'social', 'charity', 'ethical', 'renewable', 'carbon neutral', \n",
    "                   'equitable', 'ecological', 'efficient', 'improve', 'cooperative', \n",
    "                   'beneficial', 'collaborative', 'productive', 'leader', \n",
    "                   'donate', 'optimal', 'favorable', 'desirable', 'resilient', \n",
    "                   'robust', 'reasonable', 'strong', 'organic']\n",
    "\n",
    "print(\"Defined {} negative and {} positive topics\".format(len(topics_negative), len(topics_positive)))\n",
    "\n",
    "\n",
    "# create df with topics and label\n",
    "df_topics_neg = pd.DataFrame({'topic':topics_negative, 'positive': 0})\n",
    "df_topics_pos = pd.DataFrame({'topic':topics_positive, 'positive': 1})\n",
    "df_topics = pd.concat([df_topics_neg, df_topics_pos]).reset_index(drop=True)\n",
    "df_topics['date_define_topic'] = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "h.make_csv(df_sp500_wiki, 'esg_topics.csv', '../data/processed',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>positive</th>\n",
       "      <th>ticker</th>\n",
       "      <th>firm_name_raw</th>\n",
       "      <th>sector</th>\n",
       "      <th>firm_name_processed</th>\n",
       "      <th>date_get_firmname</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scandal</td>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>3M</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>scandal 3M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>greenwashing</td>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>3M</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>greenwashing 3M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>corruption</td>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>3M</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>corruption 3M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fraud</td>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>3M</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>fraud 3M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bribe</td>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>3M</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>bribe 3M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic  positive ticker firm_name_raw       sector  \\\n",
       "0       scandal         0    MMM    3M Company  Industrials   \n",
       "1  greenwashing         0    MMM    3M Company  Industrials   \n",
       "2    corruption         0    MMM    3M Company  Industrials   \n",
       "3         fraud         0    MMM    3M Company  Industrials   \n",
       "4         bribe         0    MMM    3M Company  Industrials   \n",
       "\n",
       "  firm_name_processed date_get_firmname      search_term  \n",
       "0                  3M        2020-10-17       scandal 3M  \n",
       "1                  3M        2020-10-17  greenwashing 3M  \n",
       "2                  3M        2020-10-17    corruption 3M  \n",
       "3                  3M        2020-10-17         fraud 3M  \n",
       "4                  3M        2020-10-17         bribe 3M  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# expand firm names for each topic\n",
    "df_sp500_expanded = df_sp500_wiki.iloc[np.repeat(np.arange(len(df_sp500_wiki)), len(df_topics))]\n",
    "# expand topics for each firm \n",
    "df_topics_expanded = df_topics.iloc[list(np.arange(len(df_topics)))*len(df_sp500_wiki)]\\\n",
    "    .set_index(df_sp500_expanded.index)\n",
    "\n",
    "# GENERATE search keywords as a combintation of firm name + topic\n",
    "search_terms = pd.DataFrame({'search_term':[i+' '+j for j in df_sp500_wiki.firm_name_processed for i in df_topics.topic]})\\\n",
    "    .set_index(df_sp500_expanded.index)\n",
    "\n",
    "# merge topics, firm names, and search terms into 1 df\n",
    "df_query_input = pd.concat([df_topics_expanded, df_sp500_expanded, search_terms], axis=1).reset_index(drop=True)\n",
    "\n",
    "df_query_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## API queries with: `query()` \n",
    "### handles errors, manages timeout and stores indices where errors occured "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from random import randint # for random timeout +/- 5\n",
    "\n",
    "def query(keywords, filepath, max_retries=1, idx_unsuccessful=list(), timeout=20) :\n",
    "    \"\"\"Handle failed query and handle raised exceptions\n",
    "    \n",
    "    Input\n",
    "        keywords: list with keywords for which to retrieve news\n",
    "        max_retries: number of maximum retries\n",
    "        until_page: maximum number of retrievd news page\n",
    "        \n",
    "    \n",
    "    Return\n",
    "        Inidces where max retries were reached\n",
    "    \"\"\"    \n",
    "    # retry until max_retries reached\n",
    "    for attempt in range(max_retries):   \n",
    "\n",
    "        # random int from range around timeout \n",
    "        timeout_randomized = randint(timeout-3,timeout+3)\n",
    "\n",
    "        try:\n",
    "            df_result = query_googletrends(keywords)\n",
    "\n",
    "\n",
    "        # handle query error\n",
    "        except Exception as e:\n",
    "\n",
    "            # increase timeout\n",
    "            timeout += 5\n",
    "\n",
    "            print(\">>> EXCEPTION at {}: {} \\n Set timeout to {}\\n\".format(i, e, timeout))\n",
    "            # sleep\n",
    "            h.sleep_countdown(timeout_randomized, print_step=2)\n",
    "\n",
    "\n",
    "        # query was successful: store results, sleep \n",
    "        else:\n",
    "\n",
    "            # generate timestamp for csv\n",
    "            stamp = h.timestamp_now()\n",
    "\n",
    "            # merge news dataframes and export query results\n",
    "            h.make_csv(df_result, \"gtrends.csv\", filepath, append=True)\n",
    "\n",
    "            # sleep\n",
    "            h.sleep_countdown(timeout_randomized)\n",
    "            break\n",
    "\n",
    "    # max_retries reached: store index of unsuccessful query\n",
    "    else:\n",
    "        h.make_csv(pd.DataFrame(keywords), \"unsuccessful_queries.csv\", filepath, append=True)\n",
    "        print(\"i: {} appended to idx_unsuccessful\\n\".format(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Trends Query\n",
    "\n",
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "import pandas as pd \n",
    "\n",
    "# initialize pytrends\n",
    "pt = TrendReq()\n",
    "\n",
    "# pass keywords to pytrends API \n",
    "kw_batch_null = ['desirable Zoetis', 'resilient Zoetis', 'robust Zoetis', 'reasonable Zoetis', 'strong Zoetis']\n",
    "kw_batch_success = ['cake', 'pizza']\n",
    "\n",
    "pt.build_payload(kw_list=kw_batch_success) \n",
    "\n",
    "# store results from query in df and append to df_list\n",
    "df_query_result = pt.interest_over_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query API with method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_googletrends(keywords):\n",
    "    # initialize pytrends\n",
    "    pt = TrendReq()\n",
    "    \n",
    "    # pass keywords to api\n",
    "    pt.build_payload(kw_list=keywords) \n",
    "    \n",
    "    # store results from query in df and append to df_list\n",
    "    df_query_result = pt.interest_over_time()\n",
    "    \n",
    "    # preprocess query results\n",
    "    df_query_result_processed = handle_query_results(df_query_result, keywords)\n",
    "    \n",
    "    return df_query_result_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['scandal 3M', 'greenwashing 3M', 'corruption 3M', 'fraud 3M', 'bribe 3M']\n",
      "Path created: ../data/raw/gtrends.csv\n",
      " Complete!emaining:\n",
      "1 ['greenwashing 3M', 'corruption 3M', 'fraud 3M', 'bribe 3M', 'tax 3M']\n",
      "Path created: ../data/raw/gtrends.csv\n",
      " Complete!emaining:\n"
     ]
    }
   ],
   "source": [
    "# for each batch of 5 keywords, make googletrends query \n",
    "for i in range(0,len(df_query_input)-4)[:2]:\n",
    "    kw_batch = [k for k in df_query_input.search_term[i:i+5]]\n",
    "    \n",
    "    print(i, kw_batch)\n",
    "    \n",
    "    # feed keyword batch to api query function\n",
    "    query(keywords=kw_batch, filepath='../data/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['isPartial'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-f4e0f808be2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# reset_index to preserve date information, drop isPartial column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_query_result_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_query_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'isPartial'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_query_result_long\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_query_result_processed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'keyword'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'search_interest'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3995\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3996\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3997\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3998\u001b[0m         )\n\u001b[0;32m   3999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3934\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3935\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3936\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3938\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3970\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5015\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5017\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5019\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['isPartial'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# reset_index to preserve date information, drop isPartial column\n",
    "df_query_result_processed = df_query_result.reset_index()\\\n",
    "    .drop(['isPartial'], axis=1)\n",
    "\n",
    "df_query_result_long = pd.melt(df_query_result_processed, id_vars=['date'], var_name='keyword', value_name='search_interest')\n",
    "df_query_result_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         scandal\n",
      "1    greenwashing\n",
      "2      corruption\n",
      "3           fraud\n",
      "4           bribe\n",
      "Name: topic, dtype: object\n",
      "5           tax\n",
      "6        forced\n",
      "7    harassment\n",
      "8     violation\n",
      "9       illegal\n",
      "Name: topic, dtype: object\n",
      "10          conflict\n",
      "11           weapons\n",
      "12         pollution\n",
      "13        inequality\n",
      "14    discrimination\n",
      "Name: topic, dtype: object\n",
      "15            sexism\n",
      "16            racist\n",
      "17     intransparent\n",
      "18    nontransparent\n",
      "19            breach\n",
      "Name: topic, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,16,5):\n",
    "    print(df_query_input.iloc[i:i+5,:].topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "import time # for sleep and timestamp\n",
    "import pandas as pd\n",
    "\n",
    "def googletrends_query(batched_keywords, sec_sleep=30):\n",
    "    \"\"\"Get Google trends data in a reliable way\n",
    "        if server does not respond, store results so far and retry with increased timeout\n",
    "    \n",
    "    Input\n",
    "        batched_keywords: list of keywords with chunks of five\n",
    "        temp_data: name of temporary file in ./data/ directory (defined in make_csv())\n",
    "\n",
    "    Return\n",
    "        None: stores query results as .csv in ./data/ \n",
    "    \"\"\"\n",
    "    # initialize pytrends\n",
    "    pt = TrendReq()\n",
    "    \n",
    "    # empty list to store dataframes\n",
    "    df_list = []\n",
    "    \n",
    "    ## iterate over keyword batches to obtain query results\n",
    "    for i, batch in enumerate(batched_keywords):\n",
    "    \n",
    "        # make query\n",
    "        try:\n",
    "            # pass keywords to pytrends API \n",
    "            pt.build_payload(kw_list=batch) \n",
    "\n",
    "            # store results from query in df and append to df_list\n",
    "            df_query_result = pt.interest_over_time()\n",
    "            \n",
    "            # check if empty and transpose to long format\n",
    "            df_query_result_long = handle_query_results(df_query_result, batch)\n",
    "            df_list.append(df_query_result_long)\n",
    "            \n",
    "            # wait (timeout)\n",
    "            time.sleep(sec_sleep)\n",
    "        \n",
    "        # error handling\n",
    "        except Exception as e:\n",
    "            print(\"Error {} for batch {}\".format(e, i))\n",
    "            \n",
    "            # merge results fetched so far\n",
    "            df_query_result = pd.concat(df_list)\n",
    "            \n",
    "            # store results in csv, indicating last successful batch (i-1) and timestamp\n",
    "            timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            df_filename = '{}_googletrends_batch_{}.csv'.format(timestr, i-1)            \n",
    "            make_csv(df_query_result, df_filename, 'data', index=True, header=True)\n",
    "            print(\"Store results in\", df_filename)\n",
    "            \n",
    "            # recursively call function with keyword_batches starting from i\n",
    "            # and an increased timeout by 10 seconds\n",
    "            sec_sleep += 10\n",
    "            print(\"Increased sec_sleep to {}\".format(sec_sleep))\n",
    "            google_query(batched_keywords[i:], sec_sleep+10)\n",
    "    \n",
    "    \n",
    "    ## finally store in csv\n",
    "    # merge query results\n",
    "    df_all_results = pd.concat(df_list)\n",
    "    # store results in csv, indicating last batch (i) and timestamp\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    df_filename = '{}_googletrends_batch_{}.csv'.format(timestr, i)  \n",
    "    \n",
    "    make_csv(df_all_results, df_filename, 'data', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo! Finance\n",
    "\n",
    "To complement the \"soft\" or alternaitve indicators from Google Trends, I rely on financial metrics. \n",
    "\n",
    "## Goal: Collect financial metrics of a firm.\n",
    "\n",
    "I retrieve financial indicators from Yahoo! Finance and a Python wrapper of their API called `yahooquery`. It takes symbols called \"ticker\" as input which the stock exchange uses to identify firms. For example, the ticker for the company 3M is *MMM*. \n",
    "\n",
    "Following the steps below gets relevant financial data from Yahoo! Finance.\n",
    "\n",
    "1. query all available financial data with the *ticker* from `metadata` as input\n",
    "2. select most recent date of financials\n",
    "3. handle missings\n",
    "    1. inspect and ignore columns with missings\n",
    "    2. crosscheck that no missings exists in final dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure pandas version>= 1.1.3. Your version: 1.1.3\n"
     ]
    }
   ],
   "source": [
    "# import helper functions from ../src/data\n",
    "from sys import path\n",
    "path.insert(1, '../src/data')\n",
    "import helper_functions as h\n",
    "\n",
    "from yahooquery import Ticker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "print(\"Ensure pandas version>= 1.1.3. Your version: {}\".format(pd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and export raw financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure pandas version>= 1.1.3. Your version: 1.1.3\n",
      "Tickers as input for queries:\n",
      "['MMM', 'ABT', 'ABBV', 'ABMD', 'ACN', 'ATVI', 'ADBE', 'AMD', 'AAP', 'AES', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE', 'ALXN', 'ALGN', 'ALLE', 'LNT', 'ALL', 'GOOGL', 'MO', 'AMZN', 'AMCR', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'ADI', 'ANSS', 'ANTM', 'AON', 'AOS', 'APA', 'AIV', 'AAPL', 'AMAT', 'APTV', 'ADM', 'ANET', 'AJG', 'AIZ', 'T', 'ATO', 'ADSK', 'ADP', 'AZO', 'AVB', 'AVY']\n",
      "Path created: ../data/raw/20201022-160824yahoofinance.csv\n"
     ]
    }
   ],
   "source": [
    "# metadata file in ../data/raw\n",
    "path_meta = h.get_files(\"../data/raw\", name_contains=\"*meta*\", absolute_path=False)[0]\n",
    "\n",
    "# read in firm tickers from metadata  (serves as input for queries)\n",
    "df_meta = pd.read_csv(path_meta)\n",
    "\n",
    "# get ticker\n",
    "tickers = list(df_meta.ticker.unique())\n",
    "\n",
    "# take sample for fast protoyping\n",
    "# n_sample = 10\n",
    "\n",
    "print(\"Tickers as input for queries:\")\n",
    "print(tickers) #SAMPLING [:n_sample]\n",
    "\n",
    "query = Ticker(tickers)#[:n_sample]\n",
    "\n",
    "# query all financial data\n",
    "df_fin_raw = query.all_financial_data()\n",
    "\n",
    "# store raw data\n",
    "stamp = h.timestamp_now()\n",
    "h.make_csv(df_fin_raw, stamp+'yahoofinance.csv', '../data/raw', header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load file ../data/raw\\20201022-160824yahoofinance.csv\n",
      "Loaded file has 59 unique firm ticker\n",
      "Select most recent date for each firm\n"
     ]
    }
   ],
   "source": [
    "load_file = h.get_files('../data/raw', name_contains=\"*yahoo*\" , absolute_path=False)[0]\n",
    "print(f\"Load file {load_file}\")\n",
    "\n",
    "# select most recent date\n",
    "df_fin_raw = pd.read_csv(load_file, index_col=0)\n",
    "print(\"Loaded file has {} unique firm ticker\".format(df_fin_raw.index.nunique()))\n",
    "\n",
    "print(\"Select most recent date for each firm\")\n",
    "date_most_recent = df_fin_raw.groupby(df_fin_raw.index).asOfDate.max() \n",
    "df_fin_recent = df_fin_raw[df_fin_raw.asOfDate.isin(date_most_recent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSINGS\n",
      "----------------------------------------\n",
      "Any missing in any row: 59/59 (100.0 %)\n",
      "\n",
      "Return info on column missings\n",
      "                                  missing_count       share\n",
      "CurrentDeferredTaxesLiabilities              59  100.000000\n",
      "OtherEquityInterest                          59  100.000000\n",
      "LiabilitiesHeldforSaleNonCurrent             59  100.000000\n",
      "OtherInventories                             59  100.000000\n",
      "NetIncomeExtraordinary                       59  100.000000\n",
      "...                                         ...         ...\n",
      "CommonStockEquity                             1    1.694915\n",
      "ShareIssued                                   1    1.694915\n",
      "StockholdersEquity                            1    1.694915\n",
      "TangibleBookValue                             1    1.694915\n",
      "DilutedNIAvailtoComStockholders               1    1.694915\n",
      "\n",
      "[237 rows x 2 columns]\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "# inspect missings\n",
    "df_colmiss = h.inspect_missings(df_fin_recent, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade off rows against columns\n",
      "Drop rows and modifiy original dataframe to keep more columns\n",
      "\n",
      "Keep Columns\tDrop Rows [%]\n",
      "2.0\t(0)\t0.0\n",
      "14.0\t(1)\t5.0\n",
      "20.0\t(2)\t10.0\n",
      "26.0\t(3)\t24.0\n",
      "27.0\t(4)\t25.0\n",
      "29.0\t(5)\t36.0\n",
      "30.0\t(6)\t42.0\n",
      "37.0\t(7)\t53.0\n",
      "37.0\t(8)\t56.0\n",
      "39.0\t(9)\t61.0\n",
      "41.0\t(10)\t68.0\n",
      "44.0\t(11)\t75.0\n",
      "45.0\t(12)\t81.0\n",
      "46.0\t(13)\t86.0\n",
      "49.0\t(14)\t90.0\n",
      "49.0\t(15)\t90.0\n",
      "51.0\t(16)\t92.0\n",
      "51.0\t(18)\t93.0\n",
      "51.0\t(19)\t93.0\n",
      "59.0\t(20)\t97.0\n"
     ]
    }
   ],
   "source": [
    "# count missings per column, store in series\n",
    "count_column_missing = df_fin_recent.isna().sum(axis=0).sort_values(ascending=False)\n",
    "column_missing_names = count_column_missing.index\n",
    "\n",
    "# get array of missing count with unique values to loop over\n",
    "count_column_missing_unique = count_column_missing[count_column_missing >= 0].sort_values().unique()\n",
    "\n",
    "# define until which missing count should be iterated\n",
    "max_missing = 20\n",
    "plot_x, plot_y = np.zeros(max_missing), np.zeros(max_missing)\n",
    "\n",
    "print(\"Trade off rows against columns\\nDrop rows and modifiy original dataframe to keep more columns\\n\")\n",
    "print(\"Keep Columns\\tDrop Rows [%]\")\n",
    "for arr_idx, i in enumerate(count_column_missing_unique[:max_missing]):\n",
    "    select_columns = count_column_missing[count_column_missing <= i].index\n",
    "\n",
    "    # return indices (=ticker) where the 1 missing per column occurs\n",
    "    indices_many_missings = list(df_fin_recent.loc[df_fin_recent[select_columns].isnull().any(1),:].index)\n",
    "    \n",
    "    ## compare modified vs. raw dropped missings\n",
    "    # modified df \n",
    "    df_fin_mod = df_fin_recent.drop(indices_many_missings).dropna(axis=1)\n",
    "    df_fin_nomiss = df_fin_mod\n",
    "    # raw df\n",
    "    df_raw = df_fin_recent.dropna(axis=1)\n",
    "    \n",
    "    ### Benefits of dropping x rows\n",
    "    ## comparison of dropping and keeping\n",
    "    # original \n",
    "    n_row_orig, n_col_orig = df_fin_recent.shape\n",
    "    # modified & cleaned \n",
    "    n_row_mod, n_col_mod = df_fin_mod.shape\n",
    "    # raw & cleaned\n",
    "    n_row_nomod, n_col_nomod = df_fin_raw_nomiss.shape\n",
    "    \n",
    "    ## STATISTICS\n",
    "    # dropped rows to modify df (count and %)\n",
    "    n_dropped_rows_mod = len(indices_many_missings)\n",
    "    pct_dropped_rows_mod = round(n_dropped_rows_mod/n_row_orig*100)\n",
    "    \n",
    "    # dropped cols for modified df (count and %)\n",
    "    n_dropped_cols_mod = n_col_orig - n_col_mod\n",
    "    pct_dropped_cols_mod = round(n_dropped_cols_mod/n_col_orig*100)\n",
    "    \n",
    "    # dropped cols for non-modified df (count and %)\n",
    "    n_dropped_cols_nomod = n_col_orig - n_col_nomod\n",
    "    pct_dropped_cols_nomod = round(n_dropped_cols_nomod/n_col_orig*100)\n",
    "    \n",
    "    # plot dropped rows against retained columns \n",
    "    # (x=100-pct_dropped_rows_mod, y=pct_dropped_rows_mod)\n",
    "    plot_x[arr_idx], plot_y[arr_idx] = 100-pct_dropped_cols_mod, pct_dropped_rows_mod\n",
    "    \n",
    "    \n",
    "    print(\"{}\\t({})\\t{}\".format(plot_x[arr_idx],i, plot_y[arr_idx])) \n",
    "\n",
    "\n",
    "# plot\n",
    "# plt.plot(plot_y, plot_x)\n",
    "# plt.ylabel(\"% retained columns\")\n",
    "# plt.xlabel(\"% dropped rows\")\n",
    "# plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_drop_rows_retain_columns(data, max_missing=3):\n",
    "    \"\"\"Dropping rows with many missings for certain columns \n",
    "        to keep columns\n",
    "    :param data: dataframe\n",
    "    :param max_missing: defines until which column-wise missing count should be iterated\n",
    "    :return list with indices to drop, tuple of numpy arrays for plotting: Columns to keep vs. rows dropped (%)\n",
    "    \"\"\"\n",
    "    # count missings per column, store in series\n",
    "    count_column_missing = data.isna().sum(axis=0).sort_values(ascending=False)\n",
    "    column_missing_names = count_column_missing.index\n",
    "\n",
    "    # get array of missing count with unique values to loop over\n",
    "    count_column_missing_unique = count_column_missing[count_column_missing >= 0].sort_values().unique()\n",
    "\n",
    "    # define until which column-wise missing count should be iterated\n",
    "    plot_x, plot_y = np.zeros(max_missing), np.zeros(max_missing)\n",
    "    print(\"Trade off rows against columns\\nDrop rows and modifiy original dataframe to keep more columns\\n\")\n",
    "    print(\"i\\tKeep Columns\\tDrop Rows [%]\\n\"+'-'*40)\n",
    "    \n",
    "    drop_rows = []\n",
    "    for arr_idx, i in enumerate(count_column_missing_unique[:max_missing]):\n",
    "        select_columns = count_column_missing[count_column_missing <= i].index\n",
    "\n",
    "        # return indices (=ticker) where the 1 missing per column occurs\n",
    "        indices_many_missings = list(data.loc[data[select_columns].isnull().any(1),:].index)\n",
    "        drop_rows.append(indices_many_missings)\n",
    "        \n",
    "        ## compare modified vs. raw dropped missings\n",
    "        # modified df \n",
    "        df_fin_mod = data.drop(indices_many_missings).dropna(axis=1)\n",
    "        df_fin_nomiss = df_fin_mod\n",
    "        # raw df\n",
    "        df_raw = data.dropna(axis=1)\n",
    "\n",
    "        ### Benefits of dropping x rows\n",
    "        ## comparison of dropping and keeping\n",
    "        # original \n",
    "        n_row_orig, n_col_orig = data.shape\n",
    "        # modified & cleaned \n",
    "        n_row_mod, n_col_mod = df_fin_mod.shape\n",
    "        # raw & cleaned\n",
    "        n_row_nomod, n_col_nomod = df_fin_raw_nomiss.shape\n",
    "\n",
    "        ## STATISTICS\n",
    "        # dropped rows to modify df (count and %)\n",
    "        n_dropped_rows_mod = len(indices_many_missings)\n",
    "        pct_dropped_rows_mod = round(n_dropped_rows_mod/n_row_orig*100)\n",
    "\n",
    "        # dropped cols for modified df (count and %)\n",
    "        n_dropped_cols_mod = n_col_orig - n_col_mod\n",
    "        pct_dropped_cols_mod = round(n_dropped_cols_mod/n_col_orig*100)\n",
    "\n",
    "        # dropped cols for non-modified df (count and %)\n",
    "        n_dropped_cols_nomod = n_col_orig - n_col_nomod\n",
    "        pct_dropped_cols_nomod = round(n_dropped_cols_nomod/n_col_orig*100)\n",
    "\n",
    "        # plot dropped rows against retained columns \n",
    "        # (x=100-pct_dropped_rows_mod, y=pct_dropped_rows_mod)\n",
    "        plot_x[arr_idx], plot_y[arr_idx] = 100-pct_dropped_cols_mod, pct_dropped_rows_mod\n",
    "        print(\"{}\\t{}\\t\\t{}\".format(i, plot_x[arr_idx], plot_y[arr_idx])) \n",
    "        \n",
    "\n",
    "    return drop_rows, plot_x, plot_y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "A df with missings. What do I need to know?\n",
    "1. missings across columns (missing count for each row)\n",
    "2. missings across rows (missing count for each column)\n",
    "3. How many rows are dropped without modifications?\n",
    "4. How many columns are dropped without modifications\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade off rows against columns\n",
      "Drop rows and modifiy original dataframe to keep more columns\n",
      "\n",
      "i\tKeep Columns\tDrop Rows [%]\n",
      "----------------------------------------\n",
      "0\t2.0\t\t0.0\n",
      "1\t14.0\t\t5.0\n",
      "2\t20.0\t\t10.0\n",
      "Drop rows: ['AEE', 'APA', 'AXP']\n",
      "Exclude 3 // 56 firms still available\n",
      "\n",
      "MISSINGS\n",
      "----------------------------------------\n",
      "Any missing in any row: 56/56 (100.0 %)\n",
      "\n",
      "Return info on column missings\n",
      "                                      missing_count       share\n",
      "CurrentDeferredTaxesLiabilities                  56  100.000000\n",
      "OtherInventories                                 56  100.000000\n",
      "CurrentNotesPayable                              56  100.000000\n",
      "NetIncomeExtraordinary                           56  100.000000\n",
      "OtherEquityInterest                              56  100.000000\n",
      "...                                             ...         ...\n",
      "EndCashPosition                                   1    1.785714\n",
      "InvestingCashFlow                                 1    1.785714\n",
      "TotalDebt                                         1    1.785714\n",
      "FreeCashFlow                                      1    1.785714\n",
      "NetNonOperatingInterestIncomeExpense              1    1.785714\n",
      "\n",
      "[207 rows x 2 columns]\n",
      "****************************************\n",
      "MISSINGS\n",
      "----------------------------------------\n",
      "Any missing in any row: 0/56 (0.0 %)\n",
      "\n",
      "No missings for any column.\n",
      "Awesome!\n",
      "Financial data 1 row for 1 firm with shape:\n",
      "(56, 34)\n"
     ]
    }
   ],
   "source": [
    "rows_many_missings, _,_ = inspect_drop_rows_retain_columns(df_fin_recent, max_missing=3)\n",
    "print(\"Drop rows:\", rows_many_missings[1])\n",
    "\n",
    "# select rows that cause 1 missing for certain columns\n",
    "drop_rows = rows_many_missings[1]\n",
    "\n",
    "# drop firms with missings\n",
    "df_fin_recent_clean = df_fin_recent.drop(drop_rows)\n",
    "\n",
    "print(f\"Exclude {len(drop_rows)} // {len(df_fin_recent_clean)} firms still available\\n\")\n",
    "\n",
    "# inspect missings again\n",
    "df_colmiss = h.inspect_missings(df_fin_recent_clean)\n",
    "\n",
    "# drop columns with missings\n",
    "df_fin_nomiss = df_fin_recent_clean.drop(df_colmiss.index, axis=1)\n",
    "\n",
    "# crosscheck\n",
    "h.inspect_missings(df_fin_nomiss)\n",
    "print(\"Awesome!\")\n",
    "\n",
    "print(\"Financial data 1 row for 1 firm with shape:\")\n",
    "print(df_fin_nomiss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_share</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AEE</th>\n",
       "      <td>226</td>\n",
       "      <td>0.937759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZO</th>\n",
       "      <td>179</td>\n",
       "      <td>0.742739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFL</th>\n",
       "      <td>150</td>\n",
       "      <td>0.622407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIG</th>\n",
       "      <td>141</td>\n",
       "      <td>0.585062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMP</th>\n",
       "      <td>140</td>\n",
       "      <td>0.580913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIZ</th>\n",
       "      <td>133</td>\n",
       "      <td>0.551867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AXP</th>\n",
       "      <td>131</td>\n",
       "      <td>0.543568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANTM</th>\n",
       "      <td>131</td>\n",
       "      <td>0.543568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALL</th>\n",
       "      <td>127</td>\n",
       "      <td>0.526971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>123</td>\n",
       "      <td>0.510373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        missing_count  missing_share\n",
       "symbol                              \n",
       "AEE               226       0.937759\n",
       "AZO               179       0.742739\n",
       "AFL               150       0.622407\n",
       "AIG               141       0.585062\n",
       "AMP               140       0.580913\n",
       "AIZ               133       0.551867\n",
       "AXP               131       0.543568\n",
       "ANTM              131       0.543568\n",
       "ALL               127       0.526971\n",
       "ACN               123       0.510373"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def row_missing_count(df, top_n=None):\n",
    "    \"\"\"Inspect absolute and relative missings across rows\n",
    "    Args\n",
    "        df: pandas DataFrame\n",
    "        top_n: restrict output to top_n indices with most missings across columns \n",
    "    Return\n",
    "        pandas dataframe with indices and their absolute and relative missings across columns\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df_colmiss_idx = df.T.isna().sum().sort_values(ascending=False)[:top_n]\n",
    "    df_colmiss_idx_share = df_colmiss_idx/df.shape[1]\n",
    "    \n",
    "    return pd.concat([df_colmiss_idx, df_colmiss_idx_share], axis=1, keys=['missing_count', 'missing_share'])\n",
    "\n",
    "row_missing_count(df_fin_recent, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape to long format and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: ../data/processed/yahoofinance_long.csv\n"
     ]
    }
   ],
   "source": [
    "# reshape into long format\n",
    "df_fin_long = pd.melt(df_fin_nomiss, id_vars=['asOfDate', 'periodType'], var_name='financial_var', value_name='financial_val', ignore_index=False)\n",
    "df_fin_clean = df_fin_long.rename(columns={'asOfDate': 'date_financial', 'periodType': 'financial_interval'})\n",
    "h.make_csv(df_fin_clean, 'yahoofinance_long.csv', '../data/processed', header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_missings(data, verbose=True):\n",
    "    \"\"\"Inspect missings across rows and across columns\n",
    "    \n",
    "    Args \n",
    "        data: pandas dataframe \n",
    "        \n",
    "    Returns\n",
    "        :return : dataframe with info on column missings  \n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"MISSINGS\")\n",
    "        print('-'*40)\n",
    "        \n",
    "    # check rows\n",
    "    rows_all = data.shape[0]\n",
    "    rows_nomiss = data.dropna().shape[0]\n",
    "\n",
    "    rowmiss_count = rows_all - rows_nomiss\n",
    "    rowmiss_share = rowmiss_count/rows_all*100\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Any missing in any row: {}/{} ({} %)\".format(rowmiss_count,rows_all, rowmiss_share))\n",
    "        print()\n",
    "    \n",
    "    # check columns\n",
    "    col_miss = [col for col in data.columns if data[col].isna().any()]\n",
    "    # no missings for any column\n",
    "    if not col_miss:\n",
    "        print(\"No missings for any column.\")\n",
    "    else:\n",
    "        # print share of missings for each column\n",
    "        print(\"Return info on column missings\")\n",
    "        ds_colmiss = data.loc[:,col_miss].isna().sum()\n",
    "        ds_colmiss_relative = data.loc[:,col_miss].isna().sum()/rows_all*100\n",
    "        \n",
    "        df_colmiss = pd.concat([ds_colmiss, ds_colmiss_relative], axis=1, keys=['missing_count', 'share'])\\\n",
    "                        .sort_values(\"share\", ascending=False)\n",
    "        if verbose:\n",
    "            print(df_colmiss)\n",
    "            print('*'*40)\n",
    "            \n",
    "        return df_colmiss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
