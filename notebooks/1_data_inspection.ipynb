{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inspection\n",
    "\n",
    "This notebook intends to explore the data and identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# import helper_functions.py\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src/data')\n",
    "import helper_functions as h\n",
    "\n",
    "# load metadata and googletrends search interest\n",
    "metadata_files = h.get_files('../data/raw', name_contains='*metadata*')\n",
    "gtrends_files = h.get_files('../data/raw', name_contains='*gtrends*')\n",
    "\n",
    "gtrends_column_names = ['date', 'keyword', 'search_interest']\n",
    "\n",
    "list_metadata_df = [pd.read_csv(i) for i in metadata_files]\n",
    "list_gtrends_df = [pd.read_csv(i, header=0, names=gtrends_column_names) for i in gtrends_files]\n",
    "\n",
    "# consider only first query\n",
    "df_metadata = list_metadata_df[0]\n",
    "df = list_gtrends_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core specifications\n",
    "\n",
    "### Data types\n",
    "\n",
    "The first google trends query with the filename `20201017-191627gtrends.csv` has 998324 entries and columns date, keyword and search_interest. They store data as string, string and float64 respectively. The type of *date* should match the stored date values and should be converted to date-type.\n",
    "\n",
    "The corresponding metadata `20201017-191627gtrends_metadata.csv` contains 3835 entries and 11 columns 'topic', 'positive', 'date_define_topic', 'ticker', 'firm_name_raw', 'sector', 'firm_name_processed', 'date_get_firmname', 'keyword', 'date_construct_keyword', 'date_query_googletrends'. Except for the dummy variable indicating positive or negative ESG criteria, *positive*, all columns store strings. Columns that store dates as strings should be similarly changed to appropriate datetime formats.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filenames\\n\", '-'*40)\n",
    "print(gtrends_files[0].split('\\\\')[-1])\n",
    "print(metadata_files[0].split('\\\\')[-1])\n",
    "\n",
    "print('\\nDataframe info\\n', '-'*40)\n",
    "print(df.info(), df_metadata.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptives for the main `gtrends.csv` data reveal an issue with the *date* column, where 0 occurs for some keyword queries. It arises by construction when the Google Trends query returns no measurable search interest for five consecutive keywords. The date column needs to be imputed for these cases by the query values. Future versions of the data collection script could store the queried date range in the corresponding `metadata.csv` file to impute more easily.\n",
    "\n",
    "Apart from this, there seem to be no anomalies. *Keyword* contains 261 unique values as expected since Google Trends returns weekly search interest over the last five years, plus a trajectory for the current week $5 \\text{ years} \\times 52 \\text{ weeks} + 1 \\text{ (current week)} = 261$.\n",
    "\n",
    "*Note:* I intentionally omit descriptives of the `metadata.csv` file. It contains all data about the Google Trends query when it was initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df.describe(include='all', percentiles=[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missings\n",
    "\n",
    "The main data from Google Trends does not contain missings. Nevertheless, date values with 0.0 need to be replaced for convenience.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rows\n",
    "rows_all = df.shape[0]\n",
    "rows_nomiss = df.dropna().shape[0]\n",
    "\n",
    "rowmiss_count = rows_all - rows_nomiss\n",
    "rowmiss_share = rowmiss_count/rows_all*100\n",
    "\n",
    "print(\"Missings per row: {}/{} ({} %)\".format(rowmiss_count,rows_all, rowmiss_share))\n",
    "\n",
    "# check columns\n",
    "col_miss = [col for col in df.columns if df[col].isna().any()]\n",
    "\n",
    "if not col_miss:\n",
    "    print(\"No missings for any column.\")\n",
    "    \n",
    "else:\n",
    "    print(df.loc[:,col_miss].isna().sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates\n",
    "\n",
    "The data contains 687700 duplicated rows for the *date* and *keyword* column. Google Trends returns `None` When search interest for all five keywords unmeasurable small. The `query()` function replaces values for *date* and *search_interest* with a 0.0 (float64). In contrast, a successful query returns 261 rows which is 1 more than the `query()` replaces. To be consistent, especially before imputing the date, this has to be fixed. \n",
    "\n",
    "An overview to improve data sourcing for Google Trends `query_google_trends.py` and its `handle_query_results()` method is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()].value_counts() #.describe(include='all', percentiles=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements for data collection and the `query()` method\n",
    "\n",
    "Future implementations of the `handle_query_results()` method should consider thde following:\n",
    "\n",
    "For queries that return `None` due to no search interest\n",
    "* return 261 rows for unsuccessful queries instead of 260\n",
    "* impute date with 261 date entries from successful queries. \n",
    "\n",
    "\n",
    "*Note:* Improvements are listed to be concise and avoid a re-launch of the query process which takes time due to timeouts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data inspection: Shortcut functions to inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_core_specifications(data, descriptives=False):\n",
    "    \"\"\"Inspect data types, shape and descriptives\n",
    "    \n",
    "    :param data: pandas dataframe \n",
    "    :param descriptives: boolean, print descriptive statistics (default=False)\n",
    "    :return: None\n",
    "    \"\"\"    \n",
    "    # check if data is list of dataframes\n",
    "    if isinstance(data, list):\n",
    "        for d in data:\n",
    "            print('-'*40)\n",
    "            print(d.info())\n",
    "            \n",
    "            if descriptives:\n",
    "                print('-'*40)\n",
    "                print(round(d.describe(include='all', percentiles=[])))\n",
    "            \n",
    "    else:\n",
    "        print('-'*40)\n",
    "        print(data.info())\n",
    "        \n",
    "        if descriptives:\n",
    "            print('-'*40)\n",
    "            print(round(data.describe(include='all', percentiles=[])))\n",
    "    print('*'*40)\n",
    "        \n",
    "\n",
    "def inspect_missings(data):\n",
    "    \"\"\"Inspect missings across rows and across columns\n",
    "    \n",
    "    :param data: pandas dataframe \n",
    "    :return: List with column names that contain missings \n",
    "    \"\"\"\n",
    "    print(\"MISSINGS\")\n",
    "    print('-'*40)\n",
    "    # check rows\n",
    "    rows_all = data.shape[0]\n",
    "    rows_nomiss = data.dropna().shape[0]\n",
    "\n",
    "    rowmiss_count = rows_all - rows_nomiss\n",
    "    rowmiss_share = rowmiss_count/rows_all*100\n",
    "\n",
    "    print(\"Missings per row: {}/{} ({} %)\".format(rowmiss_count,rows_all, rowmiss_share))\n",
    "    print()\n",
    "    \n",
    "    # check columns\n",
    "    col_miss = [col for col in data.columns if data[col].isna().any()]\n",
    "    # no missings for any column\n",
    "    if not col_miss:\n",
    "        print(\"No missings for any column.\")\n",
    "    else:\n",
    "        # print share of missings for each column\n",
    "        print(\"Column missings\")\n",
    "        ds_colmiss = data.loc[:,col_miss].isna().sum()\n",
    "        ds_colmiss_relative = data.loc[:,col_miss].isna().sum()/rows_all*100\n",
    "        \n",
    "        print(pd.concat([ds_colmiss, ds_colmiss_relative], axis=1, keys=['Count', 'Share (%)']))\n",
    "            \n",
    "    print('*'*40)\n",
    "    \n",
    "    return list(ds_colmiss.index)\n",
    "\n",
    "def inspect_duplicates(data):\n",
    "    \"\"\"Gives an overview of duplicate rows \n",
    "    \n",
    "    :param data: DataFrame\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 998324 entries, 0 to 998323\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   date             998324 non-null  object \n",
      " 1   keyword          998324 non-null  object \n",
      " 2   search_interest  998324 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 22.8+ MB\n",
      "None\n",
      "****************************************\n",
      "MISSINGS\n",
      "----------------------------------------\n",
      "Missings per row: 0/998324 (0.0 %)\n",
      "\n",
      "No missings for any column.\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "inspect_core_specifications(df)\n",
    "inspect_missings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSINGS\n",
      "----------------------------------------\n",
      "Missings per row: 10/10 (100.0 %)\n",
      "\n",
      "Column missings\n",
      "   Count  Share (%)\n",
      "2     10      100.0\n",
      "****************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame([np.zeros(10), np.zeros(10), np.repeat(np.nan, 10)]).T\n",
    "inspect_missings(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
