{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# import helper_functions.py\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src/data')\n",
    "import helper_functions as h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata and googletrends search interest\n",
    "metadata_files = h.get_files('../data/raw', name_contains='*metadata*')\n",
    "gtrends_files = h.get_files('../data/raw', name_contains='*gtrends*')\n",
    "\n",
    "gtrends_column_names = ['date', 'keyword', 'search_interest']\n",
    "\n",
    "list_metadata_df = [pd.read_csv(i) for i in metadata_files]\n",
    "list_gtrends_df = [pd.read_csv(i, header=None, names=gtrends_column_names) for i in gtrends_files]\n",
    "\n",
    "# consider only first query\n",
    "df_metadata = list_metadata_df[0]\n",
    "df = list_gtrends_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "The metadata and Google Trends query data will be used to do some checks and preprocess the data. The metadata serves as input for the query. Therefore, I use it as a ground truth to validate what the query returns since its output potentially suffers from connection errors or other exceptions.\n",
    "\n",
    "The first step before preprocessing is testing the input data (*df_metadata*) against the output of the Google Trends API (*df*). This is called **validation** and compares how the ideal data should look like as defined through `metadata` and what the query returned in `gtrends` data. \n",
    "\n",
    "After **validation** comes **preprocessing** where we obtain a clean dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Validating the data centers around what the query received as input (a batch of five keywords stored in `metadata`) and what it returned. The `metadata` serves as a baseline and allows to compare how many rows there *should* be or how many unique keywords exist. \n",
    "\n",
    "### 1. How many keywords are there in the metadata and in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'positive', 'date_define_topic', 'ticker', 'firm_name_raw',\n",
      "       'sector', 'firm_name_processed', 'date_get_firmname', 'keyword',\n",
      "       'date_construct_keyword', 'date_query_googletrends'],\n",
      "      dtype='object') \n",
      "\n",
      "3835 unqiue keywords in metadata\n",
      "3825 unqiue keywords in actual data\n",
      "==> 10 keywords have no search interest data\n"
     ]
    }
   ],
   "source": [
    "print(df_metadata.columns,'\\n')\n",
    "\n",
    "n_keywords_meta = df_metadata.keyword.nunique()\n",
    "n_keywords_actual = df.keyword.nunique()\n",
    "n_keywords_delta = n_keywords_meta - n_keywords_actual\n",
    "\n",
    "print(\"{} unqiue keywords in metadata\".format(n_keywords_meta))\n",
    "print(\"{} unqiue keywords in actual data\".format(n_keywords_actual))\n",
    "print(\"==> {} keywords have no search interest data\".format(n_keywords_delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List the 10 keywords that have no search interest data \n",
    "Calculate the set difference between the series with `pd.concat([df1,df2,df2])` and applying `drop_duplicates(keep=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125         lawsuit Alexion Pharmaceuticals\n",
      "1126          unfair Alexion Pharmaceuticals\n",
      "1127             bad Alexion Pharmaceuticals\n",
      "1128         problem Alexion Pharmaceuticals\n",
      "1129            hate Alexion Pharmaceuticals\n",
      "1975            issue American International\n",
      "1976    controversial American International\n",
      "1977           strike American International\n",
      "1978             scam American International\n",
      "1979          trouble American International\n",
      "Name: keyword, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat([df_metadata.keyword, df.keyword, df.keyword]).drop_duplicates(keep=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How many rows does the `gtrends` data have ideally vs. actually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each query returns 261 entries per keyword\n",
      "Ideal: 261 * 3835 = 1000935 rows. Actual: 998325. \n",
      "==> Delta of 2610 rows (ideal-actual)\n"
     ]
    }
   ],
   "source": [
    "entries_per_keyword = df.groupby('keyword').count().date.max()\n",
    "n_ideal = n_keywords_meta * entries_per_keyword\n",
    "n_actual = df.shape[0]\n",
    "print(\"Each query returns {} entries per keyword\".format(entries_per_keyword))\n",
    "print(\"Ideal: {} * {} = {} rows. Actual: {}. \".format(entries_per_keyword, n_keywords_meta, n_ideal, n_actual))\n",
    "print(\"==> Delta of {} rows (ideal-actual)\".format(n_ideal-n_actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Does each keyword have the same number of entries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual nr of entries per keyword: 261\n",
      "Success: All keywords have the same number of entries\n"
     ]
    }
   ],
   "source": [
    "entries_per_keyword_actual = df.groupby('keyword').count().date.value_counts().index[0]\n",
    "print(f\"Actual nr of entries per keyword: {entries_per_keyword_actual}\")\n",
    "if entries_per_keyword_actual == entries_per_keyword:\n",
    "    print(\"Success: All keywords have the same number of entries\")\n",
    "else:\n",
    "    print(\"Error: There is an issue with one of the keywords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Preprocessing continues in two steps. First, the date entries where 0 search interest was returned should be replaced with a correct date series of a query with positive search interest. Secondly, a test merge of  `gtrends` and `metadata` is done to ensure the datasets can be joined. \n",
    "\n",
    "### 1. for the *date* column replace 0 with the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philipp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "def series_duplicate(s, times=2, axis=0, reset_index=True):\n",
    "    \"\"\"Create a dataframe by repeating series multiple times\n",
    "    \n",
    "    :param s: pandas series\n",
    "    :param times: how often the series should be repeated \n",
    "    :param axis: repeat over rows (axis=0) or over columns (=1)\n",
    "    :param reset_index: reset index to consecutive integer\n",
    "    \n",
    "    :return : pandas DataFrame or Series with repeated Series from args \n",
    "    \"\"\"\n",
    "    df_dup = pd.concat([s] * times, axis=axis)\n",
    "    \n",
    "    if reset_index:\n",
    "        df_dup = df_dup.reset_index(drop=True)\n",
    "    \n",
    "    return df_dup \n",
    "\n",
    "# sequence of dates were inserted as placeholder, use .isin() to get them\n",
    "date_sequence = [str(i) for i in range(261)]\n",
    "date_sequence.append('0.0')\n",
    "\n",
    "# get date series of a successful query\n",
    "date_complete = pd.Series(df.date[~df.date.isin(date_sequence)].unique())\n",
    "\n",
    "# for each keyword with date series == 0.0, replace series with date_complete\n",
    "# simply expand date_complete to match n_rows of df with date == '0.0'\n",
    "\n",
    "df_zerodate = df[df.date.isin(date_sequence)]\n",
    "n_repeat = df_zerodate.keyword.unique().shape[0]\n",
    "\n",
    "date_duplicate_series = series_duplicate(date_complete, n_repeat)\n",
    "# ensure same index as df_zerodate\n",
    "date_duplicate_series.index = df_zerodate.index\n",
    "# replace date column\n",
    "df_zerodate['date'] = date_duplicate_series\n",
    "# insert back to original df\n",
    "df.loc[df_zerodate.index,:] = df_zerodate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique dates: 261\n"
     ]
    }
   ],
   "source": [
    "# unique dates should be 261\n",
    "print(\"Unique dates:\",df.date.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. take a sample and make a test merge of the main dataset and its metadata.  \n",
    "\n",
    "Each row in `metadata` (*df_metadata*) contains a keyword. In contrast, each row in `gtrends` (*df*) contains search interest per week for a keyword which repeats across dates. Thus, we have to populate `metadata` as many times as there are unique dates for each keyword which is $261$.\n",
    "\n",
    "To reduce computational expense, I take a sample of 10 keywords which corresponts to $10*261=2610$ rows in `gtrends`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>date</th>\n",
       "      <th>search_interest</th>\n",
       "      <th>topic</th>\n",
       "      <th>positive</th>\n",
       "      <th>date_define_topic</th>\n",
       "      <th>ticker</th>\n",
       "      <th>firm_name_raw</th>\n",
       "      <th>sector</th>\n",
       "      <th>firm_name_processed</th>\n",
       "      <th>date_get_firmname</th>\n",
       "      <th>date_construct_keyword</th>\n",
       "      <th>date_query_googletrends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scandal 3M</td>\n",
       "      <td>2015-10-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>scandal</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>3M</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>2020-10-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scandal 3M</td>\n",
       "      <td>2015-10-25</td>\n",
       "      <td>35.0</td>\n",
       "      <td>scandal</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>3M</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>2020-10-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword        date  search_interest    topic  positive  \\\n",
       "0  scandal 3M  2015-10-18              0.0  scandal         0   \n",
       "1  scandal 3M  2015-10-25             35.0  scandal         0   \n",
       "\n",
       "  date_define_topic ticker firm_name_raw       sector firm_name_processed  \\\n",
       "0        2020-10-17    MMM    3M Company  Industrials                  3M   \n",
       "1        2020-10-17    MMM    3M Company  Industrials                  3M   \n",
       "\n",
       "  date_get_firmname date_construct_keyword date_query_googletrends  \n",
       "0        2020-10-17             2020-10-17              2020-10-17  \n",
       "1        2020-10-17             2020-10-17              2020-10-17  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_keywords = 10\n",
    "n_dates = 261\n",
    "\n",
    "sample = n_keywords * n_dates\n",
    "\n",
    "df_sample = df.iloc[:sample,:].set_index('keyword')\n",
    "df_metadata_sample = df_metadata.iloc[:n_keywords,:].set_index('keyword')\n",
    "\n",
    "# join df and df_meta \n",
    "df_all_sample = df_sample.join(df_metadata_sample, on='keyword', how='left').reset_index()\n",
    "df_all_sample.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can pack it into a function and apply it to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_query_meta(df_query, df_meta, id_col):\n",
    "    \"\"\"Left join of df_query on df_meta, where df_meta is the input to the query\n",
    "    \n",
    "    :param df_query: pandas DataFrame with query results\n",
    "    :param df_meta: pandas DataFrame with input data for query\n",
    "    :param id_col: string that specifies the identifying column common to both dataframes\n",
    "    :return : DataFrame of joined datasets\n",
    "    \"\"\"\n",
    "    # take id as index for both\n",
    "    df_query_idcol = df_query.set_index(id_col)\n",
    "    df_meta_idcol = df_meta.set_index(id_col)\n",
    "    \n",
    "    # join query and meta\n",
    "    df_joined = df_query_idcol.join(df_meta_idcol, on=id_col, how='left').reset_index()\n",
    "    \n",
    "    return df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = join_query_meta(df, df_metadata, id_col='keyword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: ../data/processed/merged_gtrends_meta.csv\n"
     ]
    }
   ],
   "source": [
    "h.make_csv(df_all, 'merged_gtrends_meta.csv', '../data/processed', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correct storage\n",
    "df = pd.read_csv('../data/processed/merged_gtrends_meta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 998325 entries, 0 to 998324\n",
      "Data columns (total 13 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   keyword                  998325 non-null  object \n",
      " 1   date                     998325 non-null  object \n",
      " 2   search_interest          998325 non-null  float64\n",
      " 3   topic                    998325 non-null  object \n",
      " 4   positive                 998325 non-null  int64  \n",
      " 5   date_define_topic        998325 non-null  object \n",
      " 6   ticker                   998325 non-null  object \n",
      " 7   firm_name_raw            998325 non-null  object \n",
      " 8   sector                   998325 non-null  object \n",
      " 9   firm_name_processed      998325 non-null  object \n",
      " 10  date_get_firmname        998325 non-null  object \n",
      " 11  date_construct_keyword   998325 non-null  object \n",
      " 12  date_query_googletrends  998325 non-null  object \n",
      "dtypes: float64(1), int64(1), object(11)\n",
      "memory usage: 99.0+ MB\n",
      "None\n",
      "----------------------------------------\n",
      "             keyword        date  search_interest     topic  positive  \\\n",
      "count         998325      998325         998325.0    998325  998325.0   \n",
      "unique          3825         261              NaN        65       NaN   \n",
      "top     strong Adobe  2018-12-16              NaN  positive       NaN   \n",
      "freq             261        3825              NaN     15399       NaN   \n",
      "mean             NaN         NaN              2.0       NaN       0.0   \n",
      "std              NaN         NaN              9.0       NaN       0.0   \n",
      "min              NaN         NaN              0.0       NaN       0.0   \n",
      "50%              NaN         NaN              0.0       NaN       0.0   \n",
      "max              NaN         NaN            100.0       NaN       1.0   \n",
      "\n",
      "       date_define_topic  ticker              firm_name_raw  \\\n",
      "count             998325  998325                     998325   \n",
      "unique                 1      59                         59   \n",
      "top           2020-10-17    ANET  Archer-Daniels-Midland Co   \n",
      "freq              998325   16965                      16965   \n",
      "mean                 NaN     NaN                        NaN   \n",
      "std                  NaN     NaN                        NaN   \n",
      "min                  NaN     NaN                        NaN   \n",
      "50%                  NaN     NaN                        NaN   \n",
      "max                  NaN     NaN                        NaN   \n",
      "\n",
      "                        sector firm_name_processed date_get_firmname  \\\n",
      "count                   998325              998325            998325   \n",
      "unique                      11                  59                 1   \n",
      "top     Information Technology           Albemarle        2020-10-17   \n",
      "freq                    203580               16965            998325   \n",
      "mean                       NaN                 NaN               NaN   \n",
      "std                        NaN                 NaN               NaN   \n",
      "min                        NaN                 NaN               NaN   \n",
      "50%                        NaN                 NaN               NaN   \n",
      "max                        NaN                 NaN               NaN   \n",
      "\n",
      "       date_construct_keyword date_query_googletrends  \n",
      "count                  998325                  998325  \n",
      "unique                      1                       1  \n",
      "top                2020-10-17              2020-10-17  \n",
      "freq                   998325                  998325  \n",
      "mean                      NaN                     NaN  \n",
      "std                       NaN                     NaN  \n",
      "min                       NaN                     NaN  \n",
      "50%                       NaN                     NaN  \n",
      "max                       NaN                     NaN  \n",
      "****************************************\n",
      "MISSINGS\n",
      "----------------------------------------\n",
      "Missings per row: 0/998325 (0.0 %)\n",
      "\n",
      "No missings for any column.\n",
      "****************************************\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ds_colmiss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-48412f4d2454>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minspect_core_specifications\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescriptives\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minspect_missings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\GDrive\\Projekter\\Courses\\Udacity Data Engineer\\5 Capstone Project\\data_engineer_capstone\\src\\data\\helper_functions.py\u001b[0m in \u001b[0;36minspect_missings\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'*'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_colmiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'ds_colmiss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "h.inspect_core_specifications(df, descriptives=True)\n",
    "h.inspect_missings(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for data validation and preprocessing\n",
    "\n",
    "*Note*: Other helper functions accessed by `h.` reside in `../src/data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_set_difference(ds1,ds2):\n",
    "    \"\"\"Obtain the set difference of two Series\n",
    "    \n",
    "    :param ds1: Pandas Series 1\n",
    "    :param ds2: Pandas Series 2\n",
    "    :return : set_difference\n",
    "    \"\"\"\n",
    "    \n",
    "    set_difference = pd.concat([ds1, ds2, ds2]).drop_duplicates(keep=False)\n",
    "    \n",
    "    return set_difference\n",
    "\n",
    "def series_duplicate(s, times=2, axis=0, reset_index=True):\n",
    "    \"\"\"Create a dataframe by repeating series multiple times\n",
    "    \n",
    "    :param s: pandas series\n",
    "    :param times: how often the series should be repeated \n",
    "    :param axis: repeat over rows (axis=0) or over columns (=1)\n",
    "    :param reset_index: reset index to consecutive integer\n",
    "    \n",
    "    :return : pandas DataFrame or Series with repeated Series from args \n",
    "    \"\"\"\n",
    "    df_dup = pd.concat([s] * times, axis=axis)\n",
    "    \n",
    "    if reset_index:\n",
    "        df_dup = df_dup.reset_index(drop=True)\n",
    "    \n",
    "    return df_dup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1125         lawsuit Alexion Pharmaceuticals\n",
       "1126          unfair Alexion Pharmaceuticals\n",
       "1127             bad Alexion Pharmaceuticals\n",
       "1128         problem Alexion Pharmaceuticals\n",
       "1129            hate Alexion Pharmaceuticals\n",
       "1975            issue American International\n",
       "1976    controversial American International\n",
       "1977           strike American International\n",
       "1978             scam American International\n",
       "1979          trouble American International\n",
       "Name: keyword, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_set_difference(df_metadata.keyword, df.keyword)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
